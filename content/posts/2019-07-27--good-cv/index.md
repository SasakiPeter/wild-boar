---
title: 良いバリデーションとは？
category: "data-science"
cover: analysis.jpg
author: sasaki peter
---

バリデーションとは、モデルの性能を評価することである。
機械学習のモデルは、未知のデータに対する予測性能が高い程、いいモデルとされる。
真に未知のデータへの予測性能は誰も分からないため、サロゲートなエンドポイントとして、自分で未知っぽいデータを作成して、それに対する予測性能から未知のデータに対する性能を評価する。

そこには、暗黙的に自分で作成した未知っぽいデータへの予測性能と、本当に未知であるデータに対する予測性能が相関しているという前提が存在する。

しかし、実際には、これが相関している状況を作り出すのはかなり難しい。

これが相関しないような状況はどういう時が考えられるかというと、作成した未知っぽいデータに対する過学習があげられる。この場合、自分で作成した未知っぽいデータに対する予測性能が高ければ高いほど、真に未知のデータに対する予測性能は下がるということが起こりうる。

そのため、この真に未知のデータに対する予測性能を評価するためには、真に未知のデータの分布と同じような未知っぽいデータの作成や、自分で作成した未知っぽいデータに対する過学習を防ぐようなバリデーションのスキームを考える必要がある。

この自分で作成した未知っぽいデータを一般的にテストセットもしくはブラインドテストセットと呼ぶ。最初に全てのデータをトレーニングセットとテストセットに分け、トレーニングセットを用いて、モデルを作成したのち、テストセットへの予測性能から、真の未知のデータに対する予測性能を評価する。真に未知のデータに対する予測性能は汎化性能というので、今後は汎化性能という。

この最初にトレーニングセット（学習セット）とテストセット（検証セット）に分割して、性能を評価する方法をホールドアウト法というが、このホールドアウト法を一度行うだけでは、使用したバリデーションセットへの性能だけしか評価できないと不安に思い、何度も調整を繰り返しがちだが、そうするとテストセットにオーバーフィット（過学習、過適合）し、汎化性能は下がったモデルができてしまう。

したがって、このテストセットは一度しか見ないのがお約束になっている。

分析コンペにおけるこのテストセットは、コンペティション終了まで公開されないのでこういうことは起こりにくいが、研究において機械学習を行う場合は特に注意が必要だと思っている。

そこで、研究においては、その作成したテストセットがどの程度の領域をカバーしているのかというのが重要になる。

例えるならば、大学の定期試験における試験範囲がくまなく網羅されているテストセットになっているかというのが重要である。

これが偏っていた場合、その偏りの中でのモデルの性能評価になってしまう。

このモデルの適応範囲のことをApplicability Domain、通称ADという。

臨床に基づくケモインフォの領域ではこの議論が特に重要だと感じている。テストセットが広い領域をカバーしているとするには、かなり多くのデータが必要になるが、実臨床で用いられている化合物数はたかが知れているので、適応可能な範囲はとても限られたものになると思われるからである。
